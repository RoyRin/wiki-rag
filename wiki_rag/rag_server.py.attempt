from fastapi import FastAPI, Request
from pydantic import BaseModel
from typing import Optional
import uvicorn
import os
from pathlib import Path
import faiss
import sys
import torch
# local
import numpy as np
from transformers import AutoTokenizer, AutoModel
import pickle 

import encryption # local

# 🔐 Symmetric encryption key (must be securely shared after attestation)
AES_KEY = os.environ.get("RAG_AES_KEY")  # 256-bit key as base64

app = FastAPI()

# HACK: Disable encryption for now
do_encryption = False

class PromptedBGE:
    def __init__(self, model_name="BAAI/bge-base-en"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name).to(self.device).eval()

    @torch.no_grad()
    def embed(self, texts, batch_size=16):
        embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            encoded_input = self.tokenizer(batch, padding=True, truncation=True, return_tensors="pt").to(self.device)
            model_output = self.model(**encoded_input)
            emb = model_output.last_hidden_state[:, 0, :]  # CLS token embedding
            emb = torch.nn.functional.normalize(emb, p=2, dim=1)
            embeddings.append(emb.cpu().numpy())
        return np.vstack(embeddings)

    def embed_documents(self, texts):
        prompted_texts = [f"Represent this document for retrieval: {t}" for t in texts]
        return self.embed(prompted_texts)

    def embed_query(self, text):
        prompt = f"Represent this query for retrieval: {text}"
        return self.embed([prompt])[0]

BAAI_embedding = PromptedBGE(model_name="BAAI/bge-base-en")  # or bge-large-en

# 📚 Load FAISS index (in-memory)
# You'd load your document embeddings here

### LOAD RAG
FAISS_PATH = Path("/Users/roy/data/wikipedia/hugging_face/")
FAISS_PATH = FAISS_PATH / "faiss_index__top_100000__2025-04-11"
FAISS_PATH = os.environ.get("FAISS_PATH", FAISS_PATH)
print(f"FAISS_PATH {FAISS_PATH}")




def load_faiss_from_dir(path):
    path = Path(path)
    
    # Load the FAISS index
    index = faiss.read_index(str(path / "index.faiss"))
    
    # Load document IDs or metadata from pickle file
    with open(path / "index.pkl", "rb") as f:
        doc_ids = pickle.load(f)
    
    return index, doc_ids

if False:
    def load_faiss_from_dir(path):
        index = faiss.read_index(str(path / "index.faiss"))
        doc_ids = np.load(str(path / "doc_ids.npy"), allow_pickle=True)
        return index, doc_ids

# 📚 Load FAISS index (path optionally provided via CLI)
def load_vectorstore(faiss_path: Optional[str] = FAISS_PATH):
    """ adjusts global vectorstore variable """
    global faiss_index, doc_ids

    if faiss_path is None:
        default_FAISS_PATH = Path(
            "/Users/roy/data/wikipedia/hugging_face")
        faiss_path = default_FAISS_PATH / "wiki_index__top_100000__2025-04-11"
    faiss_path = Path(faiss_path)
    print(f"loaded vector store")
    index, doc_ids = load_faiss_from_dir(faiss_path)
    return index, doc_ids

    # vectorstore= FAISS.load_local(faiss_path, BAAI_embedding, allow_dangerous_deserialization=True)
    # return vectorstore


def query_index(query_text: str, top_k=5):
    query_embedding = embedding_model.embed_query(query_text)
    query_embedding = np.array([query_embedding]).astype('float32')
    faiss.normalize_L2(query_embedding)

    distances, indices = index.search(query_embedding, top_k)
    results = [(doc_ids[idx], distances[0][i]) for i, idx in enumerate(indices[0])]
    return results

# Will be set in main()
faiss_index, doc_ids = load_vectorstore()



# 🧾 Request schema
class Query(BaseModel):
    encrypted_query: str


def query_index_top(query_text,  index, doc_ids, top_k=1):
    query_embedding = embedding_model.embed_query(query_text)
    query_embedding = np.array([query_embedding]).astype('float32')
    faiss.normalize_L2(query_embedding)

    distances, indices = index.search(query_embedding, 1)
    idx = 0
    
    results = [(doc_ids[idx], distances[0][i]) for i, idx in enumerate(indices[0])]
    return results[0]

@app.post("/rag")
async def rag_endpoint(query: Query):
    global faiss_index, doc_ids
    print(f"vectorstore {faiss_index, doc_ids}")

    if do_encryption:
        print("should not be called")
        key = base64.b64decode(AES_KEY)
        user_query = encryption.decrypt_message(query.encrypted_query, key)
    else:
        user_query = query.encrypted_query  # use plaintext

    response = query_index_top(user_query, faiss_index, doc_ids, top_k=1)

    if do_encryption:
        encrypted_response = encryption.encrypt_message(response, key)
    else:
        encrypted_response = response

    return {"results": [encrypted_response]}


@app.post("/test")
async def rag_endpoint(query: Query):
    global vectorstore

    print(f"query {query}")
    return {"results": [vectorstore]}


@app.post("/provision")
async def provision_key(payload: dict):
    raw = payload["aes_key"]
    os.environ["RAG_AES_KEY"] = raw
    AES_KEY = raw
    return {"status": "ok"}


def main():
    import uvicorn
    global vectorstore
    # Optional CLI arg: FAISS path
    faiss_arg = sys.argv[1] if len(sys.argv) > 1 else None
    vectorstore = load_vectorstore(faiss_arg)
    print(f"vectorstore {vectorstore}")
    uvicorn.run("rag_server:app", host="0.0.0.0", port=8000)


if __name__ == "__main__":
    print("hello!")
    main()

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia \n",
    "import transformers\n",
    "from pathlib import Path \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "transformers.utils.logging.set_verbosity(transformers.logging.CRITICAL)\n",
    "\n",
    "\n",
    "\n",
    "# My personal cache directory\n",
    "cache_dir = Path('/n/netscratch/vadhan_lab/Lab/rrinberg/HF_cache')\n",
    "data_cache= Path(\"/n/netscratch/vadhan_lab/Lab/rrinberg/wikipedia\")\n",
    "\n",
    "if not cache_dir.exists():\n",
    "    cache_dir = None \n",
    "if not data_cache.exists():\n",
    "    data_cache = None \n",
    "    \n",
    "\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = str(cache_dir)\n",
    "\n",
    "\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = str(cache_dir)\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = str(cache_dir)\n",
    "os.environ[\"HF_HUB_CACHE\"] = str(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /n/netscratch/vadhan_lab/Lab/rrinberg/HF_cache/models--HuggingFaceH4--zephyr-7b-beta/snapshots/892b3d7a7b1cf10c7a701c60881cd93df615734c/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id- HuggingFaceH4/zephyr-7b-beta\n",
      "cache_dir - /n/netscratch/vadhan_lab/Lab/rrinberg/HF_cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected torch version: 2.5.1\n",
      "Detected torch version: 2.5.1\n",
      "loading weights file model.safetensors from cache at /n/netscratch/vadhan_lab/Lab/rrinberg/HF_cache/models--HuggingFaceH4--zephyr-7b-beta/snapshots/892b3d7a7b1cf10c7a701c60881cd93df615734c/model.safetensors.index.json\n",
      "Instantiating MistralForCausalLM model under default dtype torch.float32.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ebfe8282054185967e11c9ca6bdb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "All the weights of MistralForCausalLM were initialized from the model checkpoint at HuggingFaceH4/zephyr-7b-beta.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /n/netscratch/vadhan_lab/Lab/rrinberg/HF_cache/models--HuggingFaceH4--zephyr-7b-beta/snapshots/892b3d7a7b1cf10c7a701c60881cd93df615734c/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "loading file tokenizer.model from cache at /n/netscratch/vadhan_lab/Lab/rrinberg/HF_cache/models--HuggingFaceH4--zephyr-7b-beta/snapshots/892b3d7a7b1cf10c7a701c60881cd93df615734c/tokenizer.model\n",
      "loading file added_tokens.json from cache at /n/netscratch/vadhan_lab/Lab/rrinberg/HF_cache/models--HuggingFaceH4--zephyr-7b-beta/snapshots/892b3d7a7b1cf10c7a701c60881cd93df615734c/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /n/netscratch/vadhan_lab/Lab/rrinberg/HF_cache/models--HuggingFaceH4--zephyr-7b-beta/snapshots/892b3d7a7b1cf10c7a701c60881cd93df615734c/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /n/netscratch/vadhan_lab/Lab/rrinberg/HF_cache/models--HuggingFaceH4--zephyr-7b-beta/snapshots/892b3d7a7b1cf10c7a701c60881cd93df615734c/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at /n/netscratch/vadhan_lab/Lab/rrinberg/HF_cache/models--HuggingFaceH4--zephyr-7b-beta/snapshots/892b3d7a7b1cf10c7a701c60881cd93df615734c/tokenizer.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_debug()\n",
    "\n",
    "model_id = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "\n",
    "#model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "print(f\"model_id- {model_id}\")\n",
    "print(f\"cache_dir - {cache_dir}\")   \n",
    "device = 'cuda:0'\n",
    "dtype= torch.float32\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                 \n",
    "                                             torch_dtype=dtype, cache_dir=cache_dir,)\n",
    "model = model.to(device)\n",
    "model.requires_grad_(False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, \n",
    "                                          use_fast=False, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AccessibleComputing\n",
      "dict_keys(['id', 'title', 'url', 'text'])\n",
      "Anarchism\n",
      "dict_keys(['id', 'title', 'url', 'text'])\n",
      "45652\n",
      "Anarchism\n",
      "\n",
      "Anarchism is a political philosophy and movement that seeks to abolish all institutions that perpetuate authority, coercion, or hierarchy, primarily targeting the state and capitalism. Anarchism advocates for the replacement of the state with stateless societies and voluntary free associations. A historically left-wing movement, anarchism is usually described as the libertarian wing of the socialist movement (libertarian socialism).\n",
      "Although traces of anarchist ideas are found all throughout history, modern anarchism emerged from the Enlightenment. During the latter half of the 19th and the first decades of the 20th century, the anarchist movement flourished in most parts of the world and had a significant role in workers' struggles for emancipation. Various anarchist schools of thought formed during this period. Anarchists have taken part in several revolutions, most notably in the Paris Commune, the Russian Civil War and the Spanish Civil War, whose end marked the end of t\n",
      "AfghanistanHistory\n",
      "dict_keys(['id', 'title', 'url', 'text'])\n",
      "AfghanistanGeography\n",
      "dict_keys(['id', 'title', 'url', 'text'])\n",
      "AfghanistanPeople\n",
      "dict_keys(['id', 'title', 'url', 'text'])\n",
      "AfghanistanCommunications\n",
      "dict_keys(['id', 'title', 'url', 'text'])\n",
      "AfghanistanTransportations\n",
      "dict_keys(['id', 'title', 'url', 'text'])\n",
      "AfghanistanMilitary\n",
      "dict_keys(['id', 'title', 'url', 'text'])\n",
      "AfghanistanTransnationalIssues\n",
      "dict_keys(['id', 'title', 'url', 'text'])\n",
      "AssistiveTechnology\n",
      "dict_keys(['id', 'title', 'url', 'text'])\n",
      "AmoeboidTaxa\n",
      "dict_keys(['id', 'title', 'url', 'text'])\n",
      "Autism spectrum\n",
      "dict_keys(['id', 'title', 'url', 'text'])\n",
      "AlbaniaHistory\n",
      "dict_keys(['id', 'title', 'url', 'text'])\n",
      "AlbaniaPeople\n",
      "dict_keys(['id', 'title', 'url', 'text'])\n",
      "AsWeMayThink\n",
      "dict_keys(['id', 'title', 'url', 'text'])\n",
      "AlbaniaGovernment\n",
      "dict_keys(['id', 'title', 'url', 'text'])\n",
      "AlbaniaEconomy\n",
      "dict_keys(['id', 'title', 'url', 'text'])\n",
      "Albedo\n",
      "dict_keys(['id', 'title', 'url', 'text'])\n",
      "23865\n",
      "Albedo\n",
      "\n",
      "Albedo ( ; ) is the fraction of sunlight that is diffusely reflected by a body. It is measured on a scale from 0 (corresponding to a black body that absorbs all incident radiation) to 1 (corresponding to a body that reflects all incident radiation). \"Surface albedo\" is defined as the ratio of radiosity \"J\"e to the irradiance \"E\"e (flux per unit area) received by a surface. The proportion reflected is not only determined by properties of the surface itself, but also by the spectral and angular distribution of solar radiation reaching the Earth's surface. These factors vary with atmospheric composition, geographic location, and time (see position of the Sun).\n",
      "While directional-hemispherical reflectance factor is calculated for a single angle of incidence (i.e., for a given position of the Sun), albedo is the directional integration of reflectance over all solar angles in a given period. The temporal resolution may range from seconds (as obtained from flux measurements) to daily, \n"
     ]
    }
   ],
   "source": [
    "from importlib import reload \n",
    "\n",
    "from wiki_rag import wikipedia\n",
    "reload(wikipedia)\n",
    "from wiki_rag import rag\n",
    "\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "data_cache= Path(\"/n/netscratch/vadhan_lab/Lab/rrinberg/wikipedia/text\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i, article in enumerate(wikipedia.parse_wikiextractor_output(data_cache)):\n",
    "    print(article['title'])\n",
    "    print(article.keys())\n",
    "    \n",
    "    text = article['text']\n",
    "    if len(text) < 100:\n",
    "        continue\n",
    "    \n",
    "    # remove any leading or trailing whitespace\n",
    "    text = text.strip()\n",
    "    print(len(text))\n",
    "    print(text[:1000])  # preview first 300 characters\n",
    "    if i >10:\n",
    "        break  # remove this to keep going\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 25 articles so far...\n",
      "Processed 50 articles so far...\n",
      "Processed 75 articles so far...\n",
      "Processed 100 articles so far...\n",
      "Processed 125 articles so far...\n",
      "Processed 150 articles so far...\n",
      "Processed 175 articles so far...\n",
      "Processed 200 articles so far...\n",
      "Processed 225 articles so far...\n",
      "Processed 250 articles so far...\n",
      "Processed 275 articles so far...\n",
      "Processed 300 articles so far...\n",
      "Processed 325 articles so far...\n",
      "Processed 350 articles so far...\n",
      "Processed 375 articles so far...\n",
      "Processed 400 articles so far...\n",
      "Processed 425 articles so far...\n",
      "Processed 450 articles so far...\n",
      "Processed 475 articles so far...\n",
      "Processed 500 articles so far...\n",
      "Processed 525 articles so far...\n",
      "Processed 550 articles so far...\n",
      "Processed 575 articles so far...\n",
      "Processed 600 articles so far...\n",
      "Processed 625 articles so far...\n",
      "Processed 650 articles so far...\n",
      "Processed 675 articles so far...\n",
      "Processed 700 articles so far...\n",
      "Processed 725 articles so far...\n",
      "Processed 750 articles so far...\n",
      "Processed 775 articles so far...\n",
      "Processed 800 articles so far...\n",
      "Processed 825 articles so far...\n",
      "Processed 850 articles so far...\n",
      "Processed 875 articles so far...\n",
      "Processed 900 articles so far...\n",
      "Processed 925 articles so far...\n",
      "Processed 950 articles so far...\n",
      "Processed 975 articles so far...\n",
      "Processed 1000 articles so far...\n",
      "Processed 1025 articles so far...\n",
      "Processed 1050 articles so far...\n",
      "Processed 1075 articles so far...\n",
      "Processed 1100 articles so far...\n",
      "Processed 1125 articles so far...\n",
      "Processed 1150 articles so far...\n",
      "Processed 1175 articles so far...\n",
      "Processed 1200 articles so far...\n",
      "Processed 1225 articles so far...\n",
      "Processed 1250 articles so far...\n",
      "Processed 1275 articles so far...\n",
      "Processed 1300 articles so far...\n",
      "Processed 1325 articles so far...\n",
      "✅ FAISS index saved to wiki_faiss_index\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from pathlib import Path \n",
    "import json\n",
    "import os\n",
    "from wiki_rag import wikipedia\n",
    "from itertools import islice\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "# === Helper to batch an iterator ===\n",
    "def batched(iterable: Iterator, batch_size: int):\n",
    "    iterator = iter(iterable)\n",
    "    while batch := list(islice(iterator, batch_size)):\n",
    "        yield batch\n",
    "\n",
    "embeddings = rag.ModelEmbeddings(model, tokenizer, device)\n",
    "vectorstore = None\n",
    "\n",
    "wiki_generator = wikipedia.parse_wikiextractor_output(data_cache)\n",
    "counts = 0\n",
    "batch_size = 4\n",
    "\n",
    "docs = []\n",
    "    \n",
    "for i, d in enumerate(wiki_generator):\n",
    "    #print(f\"Processing article {counts}: {d['title']}\")\n",
    "    if i > int(2000):\n",
    "        break\n",
    "\n",
    "    title = d['title']\n",
    "    url = d['url']\n",
    "    text = d['text']\n",
    "    id_ = d.get('id')\n",
    "    if len(text) < 100:\n",
    "        continue\n",
    "    counts +=1\n",
    "    \n",
    "    if counts % 25 == 0:\n",
    "        print(f\"Processed {counts} articles so far...\")\n",
    "    \n",
    "    text = text.strip()\n",
    "    # abstract is first 3 par\n",
    "    abstract = \"\\n\".join(text.split(\"\\n\")[:3])\n",
    "    doc = Document(page_content=abstract, metadata={\"title\": title})\n",
    "    \n",
    "    if vectorstore is None:\n",
    "        vectorstore = FAISS.from_documents([doc], embeddings)\n",
    "    else:\n",
    "        vectorstore.add_documents([doc])\n",
    "\n",
    "index_path = \"wiki_faiss_index\"\n",
    "if vectorstore:\n",
    "    vectorstore.save_local(index_path)\n",
    "    print(f\"✅ FAISS index saved to {index_path}\")\n",
    "else:\n",
    "    print(\"⚠️ No documents were indexed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.search(\"Who is the author of the article on quantum computing?\", k=5, search_type=\"similarity\")\n",
    "\n",
    "#num_elements = \n",
    "vectorstore.index.ntotal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m wiki_generator \u001b[38;5;241m=\u001b[39m wikipedia\u001b[38;5;241m.\u001b[39mwikipedia_abstract_generator(data_cache)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwiki_generator\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/code/wiki-rag/wiki_rag/wikipedia.py:55\u001b[0m, in \u001b[0;36mwikipedia_abstract_generator\u001b[0;34m(path_to_extracted_dir)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m title, text \u001b[38;5;241m=\u001b[39m \u001b[43mread_article_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m title \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m text:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/code/wiki-rag/wiki_rag/wikipedia.py:30\u001b[0m, in \u001b[0;36mread_article_from_json\u001b[0;34m(full_path, offset)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n\u001b[0;32m---> 30\u001b[0m     title \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m     text \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m title, text\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "\n",
    "wiki_generator = wikipedia.wikipedia_abstract_generator(data_cache)\n",
    "\n",
    "for _ in range(3):\n",
    "    print(next(wiki_generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "# from transformers import AdamW\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss,MSELoss, NLLLoss, KLDivLoss\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
